{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import keras\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports are always needed\n",
    "import torch\n",
    "# get index of currently selected device\n",
    "torch.cuda.current_device() # returns 0 in my case\n",
    "# get number of GPUs available\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "# get the name of the device\n",
    "torch.cuda.get_device_name(0) # good old Tesla K80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# modify this to fit current environment \n",
    "\n",
    "def DQN(obs,actions):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(32, input_dim=obs, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(32, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(actions, activation = 'linear'))\n",
    "    model.compile(optimizer = 'adam', loss = 'mse',metrics=['loss'])\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "class agent():\n",
    "    def __init__(self,obs,actions):\n",
    "        self.actions = actions\n",
    "        self.batch = 128\n",
    "        self.discount_factor = 0.99\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.9954\n",
    "        # self.tau = 0.005\n",
    "        self.learning_rate = 0.1\n",
    "        self.memory = deque([], maxlen=5000) # replay memory \n",
    "        # self.policy_net = DQN(obs, actions).to(device) # action value function\n",
    "        # self.target_net = DQN(obs, actions).to(device) # target action value function \n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions)\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "        # self.Q_table = self.print_table() # not working rn\n",
    "        \n",
    "    \n",
    "    def select_action(self,env,state):\n",
    "        p = random.random()\n",
    "        if p < self.eps:\n",
    "            #random action\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            #best action\n",
    "            print(state)\n",
    "            q_table = self.policy_net.predict(state,verbose = 0)[0]\n",
    "            print(q_table)\n",
    "            return np.amax(q_table)\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(agent.memory) < self.batch:\n",
    "            return\n",
    "        else:\n",
    "            sample = random.sample(self.memory, self.batch)\n",
    "            states_list = []\n",
    "            q_values_list = []\n",
    "            for state, action, reward, next_state,terminated in sample:\n",
    "                if terminated == True:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    # next_q = self.target_net(next_state).max(1)\n",
    "                    next_q = self.target_net.predict(next_state)[0]\n",
    "                    target = (reward + self.learning_rate * np.amax(next_q))\n",
    "                q_values = self.policy_net(state,verbose = 0)[0]\n",
    "                q_values[action] = target\n",
    "                states_list.append(state)\n",
    "                q_values_list.append(q_values)\n",
    "            self.policy_net.fit(states_list,q_values_list,epochs = 1,verbose = 0)\n",
    "            return\n",
    "\n",
    "            \n",
    "        #replay algorithim here\n",
    "    # def print_table(self): # not working rn\n",
    "        \n",
    "    #     return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop end at: 12\n",
      "loop end at: 16\n",
      "[[-0.04254026 -0.17621116  0.09576438  0.41936132]]\n",
      "[-0.04353275 -0.00275847]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "np.float32(-0.002758469) (<class 'numpy.float32'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m     23\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(env,state)\n\u001b[1;32m---> 24\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# reward +=timestep\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(observation, [\u001b[38;5;241m1\u001b[39m, obs])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m    134\u001b[0m         action\n\u001b[0;32m    135\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using step method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "\u001b[1;31mAssertionError\u001b[0m: np.float32(-0.002758469) (<class 'numpy.float32'>) invalid"
     ]
    }
   ],
   "source": [
    "#create environment to run DQN\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state, info = env.reset()\n",
    "    obs = len(state)\n",
    "    actions = env.action_space.n\n",
    "    agent = agent(obs,actions)\n",
    "    max_episodes = 600\n",
    "    max_timestep = 500\n",
    "    state, info = env.reset()\n",
    "    obs = len(state)\n",
    "    actions = env.action_space.n\n",
    "    # agent = agent(obs,actions)\n",
    "    reward_per_episode = []\n",
    "    for episode in range(max_episodes):\n",
    "        # Initialize the environment and get its state\n",
    "        state, info = env.reset()\n",
    "        if episode%5 == 0 or truncated == True:\n",
    "            agent.target_net.set_weights(agent.policy_net.get_weights())\n",
    "        terminated,truncated = False, False\n",
    "        for timestep in range(500):\n",
    "            action = agent.select_action(env,state)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            # reward +=timestep\n",
    "            next_state = np.reshape(observation, [1, obs])\n",
    "            # reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n",
    "            q = [state, action, reward, next_state,terminated]\n",
    "            agent.memory.append(q)\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            if timestep % 3  == 0 :\n",
    "                agent.replay() #update network weights\n",
    "            if terminated or truncated: # close loop if it ends\n",
    "                print('loop end at:', timestep)\n",
    "                reward_per_episode.append(timestep)\n",
    "                break\n",
    "        agent.eps = agent.eps * agent.eps_decay #eps decay\n",
    "\n",
    "    print('solved',truncated)\n",
    "    plt.figure()\n",
    "    fig ,  ax = plt.subplots()\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Timestep', fontsize=20)\n",
    "    plt.title('Cumulative Reward Per Episode', fontsize=24)\n",
    "    ax.plot(reward_per_episode,linestyle='solid',label = 'Q learning')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
